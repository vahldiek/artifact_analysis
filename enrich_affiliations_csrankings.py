#!/usr/bin/env python3
"""
Enrich author affiliations using CSRankings data.

CSRankings (http://csrankings.org) maintains a comprehensive database of 
computer science faculty affiliations. This script downloads the official 
csrankings.csv file and matches our authors to their faculty records.
"""

import argparse
import csv
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Set
import requests
from collections import defaultdict

CSRANKINGS_URL = "https://raw.githubusercontent.com/emeryberger/CSrankings/gh-pages/csrankings.csv"
CACHE_DIR = Path(".cache/csrankings")
CACHE_FILE = CACHE_DIR / "csrankings.csv"
CACHE_TTL_DAYS = 30  # CSRankings data changes monthly

def download_csrankings(force_refresh: bool = False, verbose: bool = False) -> Path:
    """Download CSRankings CSV file with caching."""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    # Check cache freshness
    if CACHE_FILE.exists() and not force_refresh:
        age_days = (time.time() - CACHE_FILE.stat().st_mtime) / 86400
        if age_days < CACHE_TTL_DAYS:
            if verbose:
                print(f"Using cached CSRankings data (age: {age_days:.1f} days)")
            return CACHE_FILE
    
    # Download fresh data
    if verbose:
        print(f"Downloading CSRankings data from {CSRANKINGS_URL}...")
    
    # Support proxy environment variables
    proxies = {}
    if os.environ.get('http_proxy'):
        proxies['http'] = os.environ['http_proxy']
    if os.environ.get('https_proxy'):
        proxies['https'] = os.environ['https_proxy']
    
    try:
        response = requests.get(CSRANKINGS_URL, proxies=proxies, timeout=60)
        response.raise_for_status()
        
        CACHE_FILE.write_text(response.text, encoding='utf-8')
        if verbose:
            print(f"Downloaded {len(response.text)} bytes to {CACHE_FILE}")
        
        return CACHE_FILE
    except Exception as e:
        if CACHE_FILE.exists():
            print(f"Warning: Download failed ({e}), using stale cache", file=sys.stderr)
            return CACHE_FILE
        raise

def load_csrankings(csv_path: Path, verbose: bool = False) -> Dict[str, List[Dict]]:
    """
    Load CSRankings CSV and build name lookup index.
    Returns dict mapping normalized names to list of possible records.
    """
    name_index = defaultdict(list)
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            name = row.get('name', '').strip()
            affiliation = row.get('affiliation', '').strip()
            
            if not name or not affiliation:
                continue
            
            # Store record with multiple name variations for matching
            record = {
                'name': name,
                'affiliation': affiliation,
                'homepage': row.get('homepage', '').strip(),
                'scholarid': row.get('scholarid', '').strip(),
                'orcid': row.get('orcid', '').strip()
            }
            
            # Index by normalized full name
            normalized_name = normalize_name(name)
            name_index[normalized_name].append(record)
            
            # Also index by last name for partial matching
            parts = name.split()
            if len(parts) >= 2:
                last_name = parts[-1].lower()
                name_index[f"lastname:{last_name}"].append(record)
    
    if verbose:
        print(f"Loaded {len([r for records in name_index.values() for r in records if 'lastname:' not in r])} CSRankings records")
    
    return name_index

def normalize_name(name: str) -> str:
    """Normalize name for matching: lowercase, remove punctuation."""
    return ''.join(c.lower() for c in name if c.isalnum() or c.isspace()).strip()

def fuzzy_name_match(author_name: str, csrankings_name: str) -> bool:
    """
    Check if names match, handling common variations:
    - Middle names / initials
    - Name order (First Last vs Last, First)
    - Accents and Unicode normalization
    """
    auth_norm = normalize_name(author_name)
    cs_norm = normalize_name(csrankings_name)
    
    # Exact match
    if auth_norm == cs_norm:
        return True
    
    # Split into parts
    auth_parts = auth_norm.split()
    cs_parts = cs_norm.split()
    
    if not auth_parts or not cs_parts:
        return False
    
    # Last name must match
    if auth_parts[-1] != cs_parts[-1]:
        return False
    
    # First name match (allowing initials)
    auth_first = auth_parts[0]
    cs_first = cs_parts[0]
    
    if auth_first == cs_first:
        return True
    
    # Check if one is initial of the other
    if (len(auth_first) == 1 and cs_first.startswith(auth_first)) or \
       (len(cs_first) == 1 and auth_first.startswith(cs_first)):
        return True
    
    return False

def match_author_to_csrankings(
    author_name: str,
    name_index: Dict[str, List[Dict]],
    verbose: bool = False
) -> Optional[str]:
    """
    Match author to CSRankings record and return affiliation.
    Returns None if no match found.
    """
    normalized = normalize_name(author_name)
    
    # Try exact normalized match first
    candidates = name_index.get(normalized, [])
    
    if not candidates:
        # Try last name match
        parts = author_name.split()
        if len(parts) >= 2:
            last_name_key = f"lastname:{parts[-1].lower()}"
            candidates = name_index.get(last_name_key, [])
    
    # Find best match using fuzzy matching
    for record in candidates:
        if fuzzy_name_match(author_name, record['name']):
            if verbose:
                print(f"    Matched '{author_name}' -> '{record['name']}' ({record['affiliation']})")
            return record['affiliation']
    
    return None

def enrich_affiliations(
    authors_file: Path,
    output_file: Path,
    name_index: Dict[str, List[Dict]],
    max_authors: Optional[int] = None,
    dry_run: bool = False,
    verbose: bool = False
) -> Dict[str, int]:
    """
    Enrich author affiliations using CSRankings data.
    Returns statistics about enrichment.
    """
    # Load authors
    with open(authors_file, 'r', encoding='utf-8') as f:
        authors = json.load(f)
    
    # Track statistics
    stats = {
        'total': len(authors),
        'already_has_affiliation': 0,
        'csrankings_match': 0,
        'no_match': 0,
        'enriched': 0
    }
    
    # Find authors missing affiliations
    missing_affiliation = [
        author for author in authors 
        if not author.get('affiliation') or author['affiliation'] == 'Unknown'
    ]
    
    stats['already_has_affiliation'] = stats['total'] - len(missing_affiliation)
    
    if max_authors:
        missing_affiliation = missing_affiliation[:max_authors]
    
    print(f"Processing {len(missing_affiliation)} authors with missing affiliations...")
    
    # Enrich affiliations
    enriched_count = 0
    for i, author in enumerate(missing_affiliation, 1):
        name = author.get('name', '')
        
        if verbose:
            print(f"  [{i}/{len(missing_affiliation)}] Looking up: {name}")
        
        affiliation = match_author_to_csrankings(name, name_index, verbose)
        
        if affiliation:
            author['affiliation'] = affiliation
            enriched_count += 1
            stats['csrankings_match'] += 1
            if verbose:
                print(f"    ✓ Found: {affiliation}")
            
            # Progress update every 100 authors
            if not verbose and i % 100 == 0:
                print(f"  Processed {i}/{len(missing_affiliation)}... (found {enriched_count} so far)")
        else:
            stats['no_match'] += 1
            if verbose:
                print(f"    ✗ No match in CSRankings")
    
    stats['enriched'] = enriched_count
    
    # Save results
    if not dry_run:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(authors, f, indent=2, ensure_ascii=False)
        print(f"\nEnriched authors saved to: {output_file}")
    else:
        print(f"\nDry run - would save to: {output_file}")
    
    return stats

def main():
    parser = argparse.ArgumentParser(
        description="Enrich author affiliations using CSRankings data"
    )
    parser.add_argument(
        '--authors_file',
        type=Path,
        default=Path('authors.json'),
        help='Input authors JSON file (default: authors.json)'
    )
    parser.add_argument(
        '--output',
        type=Path,
        default=Path('authors_enriched_csrankings.json'),
        help='Output file (default: authors_enriched_csrankings.json)'
    )
    parser.add_argument(
        '--max_authors',
        type=int,
        help='Maximum authors to process (for testing)'
    )
    parser.add_argument(
        '--force_refresh',
        action='store_true',
        help='Force download of fresh CSRankings data'
    )
    parser.add_argument(
        '--dry_run',
        action='store_true',
        help='Do not save output file'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Print detailed progress'
    )
    
    args = parser.parse_args()
    
    # Download CSRankings data
    csv_path = download_csrankings(
        force_refresh=args.force_refresh,
        verbose=args.verbose
    )
    
    # Load and index CSRankings
    name_index = load_csrankings(csv_path, verbose=args.verbose)
    
    # Enrich affiliations
    stats = enrich_affiliations(
        authors_file=args.authors_file,
        output_file=args.output,
        name_index=name_index,
        max_authors=args.max_authors,
        dry_run=args.dry_run,
        verbose=args.verbose
    )
    
    # Print summary
    print("\n" + "="*60)
    print("CSRankings Enrichment Summary")
    print("="*60)
    print(f"Total authors:              {stats['total']:,}")
    print(f"Already have affiliation:   {stats['already_has_affiliation']:,}")
    print(f"Missing affiliation:        {stats['total'] - stats['already_has_affiliation']:,}")
    print(f"CSRankings matches:         {stats['csrankings_match']:,}")
    print(f"No match found:             {stats['no_match']:,}")
    print(f"Total enriched:             {stats['enriched']:,}")
    
    if stats['total'] - stats['already_has_affiliation'] > 0:
        match_rate = 100 * stats['csrankings_match'] / (stats['total'] - stats['already_has_affiliation'])
        print(f"Match rate:                 {match_rate:.1f}%")
    
    remaining = stats['total'] - stats['already_has_affiliation'] - stats['enriched']
    final_coverage = 100 * (stats['already_has_affiliation'] + stats['enriched']) / stats['total']
    print(f"Final coverage:             {final_coverage:.1f}%")
    print(f"Still missing:              {remaining:,} ({100*remaining/stats['total']:.1f}%)")
    print("="*60)

if __name__ == '__main__':
    main()
